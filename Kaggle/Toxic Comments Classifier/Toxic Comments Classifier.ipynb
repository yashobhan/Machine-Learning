{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import  numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn import pipeline\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU: ', tpu.master())\n    \nexcept Exception as e:\n    tpu = None\n    \n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of clusters: ', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[:209999, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions_dict = {     \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I had\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"iit will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they had\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean(text, contractions=contractions_dict, remove_stop=False):\n    text = text.lower()\n    text = re.sub(r'\\([^)]*\\)', '', text)\n    text = ' '.join([contractions[t] if t in contractions else t for t in text.split(' ')])\n    text = re.sub(r\"'s\\b\", \"\", text)\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    text = re.sub('[m]{2, }', 'mm', text)\n    \n    return ' '.join(text.strip().split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('* BEFORE CLEANING: ', train.comment_text.iloc[0], '\\n')\nprint('* AFTER CLEANING: ', clean(train.comment_text.iloc[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text_clean'] = train['comment_text'].progress_apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = int(train['comment_text_clean'].str.split().apply(len).describe()['75%'] + 26)\nprint('Max comment length: ', max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(list(train['comment_text_clean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_seq = tokenizer.texts_to_sequences(train['comment_text'])\nX_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=max_len)\n\ny_train = train.toxic.values\nX_train_pad.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting up a callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A basic RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nwith strategy.scope():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(max_len, )))\n    model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 300, input_length=max_len))\n    model.add(tf.keras.layers.SimpleRNN(512))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_pad, y_train, \n          epochs=5, \n          validation_split=0.2, \n          batch_size=50*strategy.num_replicas_in_sync,\n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(model.history.history['accuracy'])\nplt.title('Simple RNN - Training Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(model.history.history['val_accuracy'])\nplt.title('Simple RNN - Testing Accuracy')\n\nplt.subplot(2, 2, 3)\nplt.plot(model.history.history['loss'], c='r')\nplt.title('Simple RNN - Training Loss')\n\nplt.subplot(2, 2, 4)\nplt.plot(model.history.history['val_loss'], c='r')\nplt.title('Simple RNN - Testing Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_file = '/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\nglove = np.load(glove_file, allow_pickle=True)\n\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\nfor word, i in tqdm(tokenizer.word_index.items()):\n    embedding_vector = glove.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using stacked LSTMs with pre-trained word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1,\n                                        embedding_matrix.shape[1],\n                                        weights=[embedding_matrix],\n                                        input_length=max_len,\n                                        trainable=False))\n    \n    model.add(tf.keras.layers.LSTM(512,\n                                   return_sequences=True,\n                                   recurrent_dropout=0.3))\n    model.add(tf.keras.layers.LSTM(512, \n                                   recurrent_dropout=0.3))\n    \n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n                  metrics=['accuracy'])\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_pad, y_train, \n          epochs=10, \n          validation_split=0.2, \n          batch_size=500*strategy.num_replicas_in_sync,\n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(model.history.history['accuracy'])\nplt.title('LSTM - Training Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(model.history.history['val_accuracy'])\nplt.title('LSTM - Testing Accuracy')\n\nplt.subplot(2, 2, 3)\nplt.plot(model.history.history['loss'], c='r')\nplt.title('LSTM - Training Loss')\n\nplt.subplot(2, 2, 4)\nplt.plot(model.history.history['val_loss'], c='r')\nplt.title('LSTM - Testing Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using stacked GRUs"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1,\n                                        embedding_matrix.shape[1],\n                                        weights=[embedding_matrix],\n                                        input_length=max_len,\n                                        trainable=False))\n    \n    model.add(tf.keras.layers.GRU(512, recurrent_dropout=0.3, return_sequences=True))\n    model.add(tf.keras.layers.GRU(512))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n                  metrics=['accuracy'])\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_pad, y_train, \n          epochs=10, \n          validation_split=0.2, \n          batch_size=500*strategy.num_replicas_in_sync,\n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(model.history.history['accuracy'])\nplt.title('GRU - Training Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(model.history.history['val_accuracy'])\nplt.title('GRU - Testing Accuracy')\n\nplt.subplot(2, 2, 3)\nplt.plot(model.history.history['loss'], c='r')\nplt.title('GRU - Training Loss')\n\nplt.subplot(2, 2, 4)\nplt.plot(model.history.history['val_loss'], c='r')\nplt.title('GRU - Testing Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using a Bi-LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1,\n                                        embedding_matrix.shape[1],\n                                        weights=[embedding_matrix],\n                                        input_length=max_len,\n                                        trainable=False))\n    \n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, recurrent_dropout=0.3)))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n                  metrics=['accuracy'])\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_pad, y_train, \n          epochs=5, \n          validation_split=0.2, \n          batch_size=60*strategy.num_replicas_in_sync,\n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(model.history.history['accuracy'])\nplt.title('Bi-LSTM - Training Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(model.history.history['val_accuracy'])\nplt.title('Bi-LSTM - Testing Accuracy')\n\nplt.subplot(2, 2, 3)\nplt.plot(model.history.history['loss'], c='r')\nplt.title('Bi-LSTM - Training Loss')\n\nplt.subplot(2, 2, 4)\nplt.plot(model.history.history['val_loss'], c='r')\nplt.title('Bi-LSTM - Testing Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using a Seq2Seq Model with Attention"},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest_data['comment_text_clean'] = test_data['comment_text'].apply(clean)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=512, maxlen=max_len):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encodings = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encodings])\n        \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO_TUNE = tf.data.experimental.AUTOTUNE\nEPOCHS = 10\nBATCH_SIZE = 120 * strategy.num_replicas_in_sync\nMAX_LEN = max_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\ntokenizer.save_pretrained('.')\n\nfast_tokenizer = transformers.BertTokenizerFast('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = fast_encode(train.comment_text_clean.astype(str), fast_tokenizer.tokenizer, maxlen=MAX_LEN)\ny_train = train.toxic.values\n\nX_test = fast_encode(test_data.comment_text_clean.astype(str), fast_tokenizer.tokenizer, maxlen=MAX_LEN)\ny_test = test_data.toxic.values\n\nX_train[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n                .from_tensor_slices((X_train, y_train))\n                .repeat()\n                .shuffle(2048)\n                .batch(BATCH_SIZE)\n                .prefetch(AUTO_TUNE))\n\ntest_dataset = (tf.data.Dataset\n                .from_tensor_slices((X_test, y_test))\n                .repeat()\n                .shuffle(2048)\n                .cache()\n                .batch(BATCH_SIZE)\n                .prefetch(AUTO_TUNE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=MAX_LEN):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len, ), dtype=tf.int32, name='input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n    \n    model = tf.keras.models.Model(inputs=input_word_ids, outputs=out)\n    \n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    \n    model = build_model(transformer_layer, max_len=MAX_LEN)\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_train.shape[0] // BATCH_SIZE\nhistory = model.fit(train_dataset,\n                    steps_per_epoch=n_steps,\n                    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_test.shape[0] // BATCH_SIZE\nhistory_test = model.fit(test_dataset,\n                    steps_per_epoch=n_steps,\n                    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.title('DistilBERT - Training Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(history.history['loss'])\nplt.title('DistilBERT - Training Loss')\n\nplt.subplot(2, 2, 3)\nplt.plot(history_test.history['accuracy'], c='r')\nplt.title('DistilBERT - Testing Accuracy')\n\nplt.subplot(2, 2, 4)\nplt.plot(history_test.history['loss'], c='r')\nplt.title('DistilBERT - Testing Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}